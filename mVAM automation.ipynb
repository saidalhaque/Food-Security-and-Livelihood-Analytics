{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ae4ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sl DataToDropFromRaw       KeepInAnalyzed     ExistingName  \\\n",
      "0      1         starttime  DataCollection_Date             Date   \n",
      "1      2           endtime        DIVISION_Name  ADMIN1_Division   \n",
      "2      3             today        DISTRICT_Name  ADMIN2_District   \n",
      "3      4          deviceid         UPAZILA_Name   ADMIN3_Upazila   \n",
      "4      5             simid  Area_RuralUrbanCity     HHurbanRural   \n",
      "..   ...               ...                  ...              ...   \n",
      "130  131               NaN                  NaN     HHConcern/12   \n",
      "131  132               NaN                  NaN     HHConcern/13   \n",
      "132  133               NaN                  NaN     HHConcern/14   \n",
      "133  134               NaN                  NaN     HHConcern/15   \n",
      "134  135               NaN                  NaN    HHConcern/-88   \n",
      "\n",
      "                           NewName  \n",
      "0              DataCollection_Date  \n",
      "1                    DIVISION_Name  \n",
      "2                    DISTRICT_Name  \n",
      "3                     UPAZILA_Name  \n",
      "4              Area_RuralUrbanCity  \n",
      "..                             ...  \n",
      "130  HHConcernHouseholdItemsUtilit  \n",
      "131  HHConcernInformationOnHumanit  \n",
      "132               HHConcernMoney14  \n",
      "133         HHConcernNotrequired15  \n",
      "134              HHConcernOthers88  \n",
      "\n",
      "[135 rows x 5 columns]\n",
      "int64\n",
      "float64\n",
      "Shape of the data: (4437, 216)\n",
      "Shape of the data: (1192, 216)\n",
      "Shape of the data: (1181, 216)\n",
      "['DataCollection_Date', 'DIVISION_Name', 'DISTRICT_Name', 'UPAZILA_Name', 'Area_RuralUrbanCity', 'Resp_Sex', 'Res_Age', 'Res_Relation_HHHead', 'HHHead_Sex', 'HHHead_Age', 'HHHead_Education', 'HH_Size', 'HHSize_Elderly65yrs', 'HHSize_adults1864yrs', 'HHSize_children517yrs', 'HHSize_children_under5yrs', 'Hhmemcheck', 'HHleveldisability_adult', 'HHlevel_Num_PWD', 'HHleveldisability_type', 'disability_type_eyesight1', 'disability_type_hearing2', 'disability_type_walking3', 'disability_type_remembering4', 'disability_type_selfcare5', 'disability_type_speaking6', 'FCSStap', 'FCSPulseNut', 'FCSDairy', 'FCSPr', 'FCSPrMeatF', 'FCSPrMeatO', 'FCSPrFish', 'FCSDriedFish', 'FCSPrEgg', 'FCSVeg', 'FCSVegOrg', 'FCSVegGre', 'FCSFruit', 'FCSFruitOrg', 'FCSFruitsOth', 'FCSFat', 'FCSSugar', 'FCSScore', 'HHFoodSource_main', 'comments.1', 'HH_mainincomesource', 'HH_2ndincomesource', 'HH_incomechangetype', 'HHIncChTypeDec', 'HHIncChTypeInc', 'HH_incomechangereason', 'HHIncCh_ReasonCouldNotWorkDu1', 'HHIncCh_ReasonBusinessClosed2', 'HHIncCh_ReasonDisruptionsInMa3', 'HHIncCh_ReasonLessCustomersCl4', 'HHIncCh_ReasonHouseholdMembers5', 'HHIncCh_ReasonLossOfEmploymen6', 'HHIncCh_ReasonReducedSalaryWa7', 'HHIncCh_ReasonDailyLabourOppo8', 'HHIncCh_ReasonSupportAssistanc9', 'HHIncCh_ReasonDisasterFloodc10', 'HHIncCh_ReasonOther88', 'HH_expchangetype', 'HHExpdChTypedec', 'HHExpdChTypeInc', 'HH_debts', 'HWaterSRC', 'comments.2', 'HH_memberssick_14days', 'HHNeedHealthAccess_14D', 'HHChallengeHealthAccess_14D', 'HHHealthConstr_14D', 'comments.3', 'HHShock_6M', 'HHShock_6MNo0', 'HHShock_6MLossofemploymentre', 'HHShock_6MSicknesshealthexp', 'HHShock_6MDeathofhouseholdme', 'HHShock_6MHighfoodprices4', 'HHShock_6MHighfuelpricetrans', 'HHShock_6MNaturalhazardsfloo', 'HHShock_6MPoorharvest7', 'HHShock_6MElectricitygascuts', 'HHShock_6MInsecuritythefts9', 'HHShock_6MIrregularunsafedrin', 'HHShock_6MLackofaccesstocre', 'HHShock_6MDebttoreinburse12', 'HHShock_6MRentpayment13', 'HHShock_6MEnvironmentalproblem', 'HHShock_6MOthers88', 'HHDisasterType', 'Cyclone1', 'FloodsFlashfloods2', 'Landslide3', 'Stormsurge4', 'Drought5', 'Windstorm6', 'RiverBankErosion7', 'Salinity8', 'Heatwave9', 'Ligthining10', 'Others88', 'Severity_HH', 'impact_HH', 'Personalinjuriesdeath1', 'HousholdShelterdamages2', 'Waterlogging3', 'HousholdShelterdamaged4', 'Livelihoodsourcesaffectedlos5', 'Limitedaccesstoserviceseduc6', 'Reducedaccesstosafewater7', 'Reducedaccesstosafesanitatio8', 'Householddisplacedtemporarily9', 'Householdmigratedpermanently10', 'HHDisasterimpactOthers88', 'comments.4', 'LcsEN_stress_DomAsset', 'LcsEN_stress_CrdtFood', 'LcsEN_stress_BorrowCash', 'LcsEN_stress_Saving', 'LcsEN_crisis_ProdAsset', 'LcsEN_crisis_AgriInput', 'LcsEN_crisis_Seed', 'LcsEN_crisis_HealthEdu', 'LcsEN_em_Begged', 'LcsEN_em_ResAsset', 'LcsEN_em_IllegalAct', 'LcsEN_em_Migrate', 'comments.5', 'rCSILessQlty', 'rCSIBorrow', 'rCSIMealSize', 'rCSIMealNb', 'rCSIMealAdult', 'comments.6', 'HH_marketaccess_14days', 'HH_marketaccesswhy_14days', 'Foodpricechange', 'Shoppingbehaviourchanage', 'comments.7', 'HH_receivedfoodlivelihoodassi', 'TypeAssistance', 'TypeAssistance/1', 'TypeAssistance/2', 'TypeAssistance/3', 'TypeAssistance/4', 'TypeAssistance/5', 'TypeAssistance/6', 'TypeAssistance/7', 'TypeAssistance/8', 'TypeAssistance/9', 'TypeAssistance/10', 'TypeAssistance/11', 'TypeAssistance/12', 'TypeAssistance/13', 'TypeAssistance/14', 'TypeAssistance/15', 'TypeAssistance/16', 'TypeAssistance/17', 'TypeAssistance/-88', 'TypeAssistance/-99', 'HHAsstFoodWFP_YN', 'comments.8', 'HH_main3needs', 'HHConcernFood1', 'HHConcernLivelihood2', 'HHConcernHealth3', 'HHConcernWater4', 'HHConcernSanitationHygiene5', 'HHConcernEducation6', 'HHConcernProtectionfromNatura', 'HHConcernSafetyAndSecurity8', 'HHConcernProtectionGenderBase', 'HHConcernShelter10', 'HHConcernCookingFuel11', 'HHConcernHouseholdItemsUtilit', 'HHConcernInformationOnHumanit', 'HHConcernMoney14', 'HHConcernNotrequired15', 'HHConcernOthers88', 'comments.9']\n",
      "HH_incomechange_aux\n",
      "2    647\n",
      "3    292\n",
      "4    118\n",
      "1    100\n",
      "5     24\n",
      "Name: count, dtype: int64\n",
      "CARI_Inc_aux\n",
      "1    444\n",
      "2    388\n",
      "3    264\n",
      "4     85\n",
      "Name: count, dtype: int64\n",
      "FGVitA\n",
      "6.0     53\n",
      "9.0     47\n",
      "5.0     47\n",
      "4.0     45\n",
      "8.0     44\n",
      "10.0    38\n",
      "7.0     36\n",
      "11.0    35\n",
      "12.0    26\n",
      "14.0    23\n",
      "16.0    19\n",
      "17.0    17\n",
      "15.0    16\n",
      "13.0    15\n",
      "18.0    12\n",
      "3.0     12\n",
      "2.0      4\n",
      "25.0     4\n",
      "19.0     4\n",
      "21.0     3\n",
      "23.0     3\n",
      "22.0     3\n",
      "20.0     2\n",
      "29.0     1\n",
      "24.0     1\n",
      "Name: count, dtype: int64\n",
      "FGProtein\n",
      "5.0     190\n",
      "6.0     154\n",
      "7.0     131\n",
      "8.0     105\n",
      "4.0      87\n",
      "9.0      70\n",
      "11.0     61\n",
      "10.0     57\n",
      "14.0     42\n",
      "3.0      40\n",
      "12.0     40\n",
      "13.0     31\n",
      "15.0     29\n",
      "16.0     27\n",
      "18.0     20\n",
      "17.0     16\n",
      "2.0      12\n",
      "21.0      9\n",
      "19.0      7\n",
      "23.0      6\n",
      "1.0       5\n",
      "20.0      4\n",
      "22.0      3\n",
      "29.0      1\n",
      "24.0      1\n",
      "Name: count, dtype: int64\n",
      "FGHIron\n",
      "2.0     283\n",
      "3.0     272\n",
      "4.0     167\n",
      "7.0     126\n",
      "5.0     107\n",
      "1.0      77\n",
      "6.0      49\n",
      "8.0      24\n",
      "0.0      21\n",
      "9.0      10\n",
      "10.0      8\n",
      "11.0      3\n",
      "12.0      1\n",
      "Name: count, dtype: int64\n",
      "FGVitACat\n",
      "7 days      349\n",
      "1-6 days    161\n",
      "0 days        0\n",
      "Name: count, dtype: int64\n",
      "FGProteinCat\n",
      "7 days      660\n",
      "1-6 days    483\n",
      "0 days        5\n",
      "Name: count, dtype: int64\n",
      "FGHIronCat\n",
      "1-6 days    878\n",
      "7 days      172\n",
      "0 days       98\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Directory containing the files\n",
    "dir_raw = \"C:/Users/saidul.haq/OneDrive - World Food Programme/Saidul/1.mVAM dashboard project/BD mVAM data/2024 raw\"\n",
    "dir_analyzed = \"C:/Users/saidul.haq/OneDrive - World Food Programme/Saidul/1.mVAM dashboard project/BD mVAM data/analyzed\"\n",
    "dir_main = \"C:/Users/saidul.haq/OneDrive - World Food Programme/Saidul/1.mVAM dashboard project/BD mVAM data\"\n",
    "# Improt Variable Mapping Workbook \n",
    "VarMapping = \"C:/Users/saidul.haq/OneDrive - World Food Programme/Saidul/1.mVAM dashboard project/automation/variableMappingWorkbook.xlsx\"\n",
    "# Read Excel data into a DataFrame\n",
    "VarMapping = pd.read_excel(VarMapping, sheet_name='Sheet1')\n",
    "# Print the data\n",
    "print(VarMapping)\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_file = \"C:/Users/saidul.haq/OneDrive - World Food Programme/Saidul/1.mVAM dashboard project/BD mVAM data/2024 raw/Round26Sep2024.xlsx\"\n",
    "\n",
    "# Read Excel data into a DataFrame\n",
    "data = pd.read_excel(excel_file, sheet_name='data')\n",
    "# See the variable types and shape of data\n",
    "print(data['CallDispo'].dtypes)\n",
    "print(data['consent'].dtypes)\n",
    "# Print the shape of the data\n",
    "print(\"Shape of the data:\", data.shape)\n",
    "\n",
    "# Convert 'consent' column from float64 to integer64\n",
    "data['consent']= data['consent'].astype('Int64')\n",
    "\n",
    "# filter the data where we drop all not responded rows\n",
    "data1 = data[(data['CallDispo']==1) & (data['consent']==1)]\n",
    "\n",
    "# Print the shape of the data1\n",
    "print(\"Shape of the data:\", data1.shape)\n",
    "\n",
    "# Keep only unique IDs in the 'UiD_phonebook' column\n",
    "data1 = data1.drop_duplicates(subset=['UiD_phonebook'])\n",
    "\n",
    "# Print the shape of the data1\n",
    "print(\"Shape of the data:\", data1.shape)\n",
    "\n",
    "# Now drop the unnessary VArible from the data\n",
    "# Step 2: Extract variable names from the Excel file\n",
    "variables_drop_row = VarMapping['DataToDropFromRaw'].dropna().tolist() \n",
    "data1 = data1.drop(columns=variables_drop_row, errors='ignore')\n",
    "\n",
    "# drop comments column\n",
    "columns = data1.columns.tolist()\n",
    "drop_com = [col for col in columns if 'comments.' in col]\n",
    "data1.drop(columns=drop_com)\n",
    "\n",
    "rename_mapping = dict(zip(VarMapping['ExistingName'], VarMapping['NewName']))\n",
    "# Rename varible in the data\n",
    "data1.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "print(data1.columns.tolist())\n",
    "data1['HH_mainincomesource'].value_counts()\n",
    "\n",
    "name = {\n",
    "    10: \"Barishal\",\n",
    "    20: \"Chattogram\",\n",
    "    30: \"Dhaka\",\n",
    "    40: \"Khulna\",\n",
    "    45: \"Mymensingh\",\n",
    "    50: \"Rajshahi\",\n",
    "    55: \"Rangpur\",\n",
    "    60: \"Sylhet\"\n",
    "}\n",
    "# Mapping the labels\n",
    "data1['DIVISION_Name'] = data1['DIVISION_Name'].map(name)\n",
    "\n",
    "data1['DIVISION_Name'].value_counts()\n",
    "\n",
    "# creat Incomegroup variables\n",
    "data1['Incomegroup'] = data1['HH_mainincomesource'].replace({\n",
    "    1:1, 2:1, 5:1,\n",
    "    3:2, 4:2, 12:2, 13:2, 17:2,\n",
    "    6:3, 7:3, 8:3, 9:3, 10:3, 11:3, 14:3, 15:3, 16:3, -88:3\n",
    "}).map({1: \"High Income Group\", 2: \"Medium Income Group\", 3: \"Low Income Group\"})\n",
    "data1['Incomegroup'].value_counts()\n",
    "\n",
    "# Household Income Change Type\n",
    "income = {1: 'Income decreased', 2:'Income increased', 3:'No Change'}\n",
    "data1['HH_incomechangetype']= data1['HH_incomechangetype'].replace(income)\n",
    "data1['HH_incomechangetype'].value_counts()\n",
    "\n",
    "# Household Expenditure Change Type\n",
    "expenditure = {1: 'Expenditure decreased', 2: 'Expenditure increased', 3:'No change'}\n",
    "data1['HH_expchangetype'] = data1['HH_expchangetype'].replace(expenditure)\n",
    "data1['HH_expchangetype'].value_counts()\n",
    "\n",
    "# compute FCSCat VAriable\n",
    "columns_to_recode = ['FCSStap', 'FCSVeg', 'FCSFruit', 'FCSPr', 'FCSPulseNut', 'FCSDairy', 'FCSFat', 'FCSSugar']\n",
    "# Recode the specified columns\n",
    "data[columns_to_recode] = data[columns_to_recode].replace({'.': 0})\n",
    "\n",
    "# Compute FCS\n",
    "data1['FCS'] = (data1['FCSStap']*2)+ (data1['FCSVeg']*1)+ (data['FCSFruit'] * 1) + (data['FCSPr'] * 4) + (data['FCSPulseNut'] * 3) + (data['FCSDairy'] * 4) + (data['FCSFat'] * 0.5) + (data['FCSSugar'] * 0.5)\n",
    "\n",
    "data1['FCSCat_aux']=0\n",
    "\n",
    "data1.loc[data1['FCS']<=28, 'FCSCat_aux']=1\n",
    "data1.loc[(data1['FCS']>28) & (data1['FCS']<=42), 'FCSCat_aux']=2\n",
    "data1.loc[data1['FCS']>42, 'FCSCat_aux']=3\n",
    "\n",
    "fcs = {1: 'Poor', 2: 'Borderline', 3: 'Acceptable'}\n",
    "\n",
    "data1['FCSCat'] = data1['FCSCat_aux'].replace(fcs)\n",
    "data1['FCSCat'].value_counts()\n",
    "\n",
    "\n",
    "# Compute rCSI\n",
    "data1['rCSI']=  (data1['rCSILessQlty']*1) + (data1['rCSIBorrow']*2) + (data1['rCSIMealNb']*1) + (data1['rCSIMealSize']*1) + (data1['rCSIMealAdult']*3)\n",
    "\n",
    "data1['rCSICat_aux']=0\n",
    "data1.loc[data1['rCSI']<= 3, 'rCSICat_aux']=1\n",
    "data1.loc[(data1['rCSI']>3) & (data1['rCSI']<=18), 'rCSICat_aux']=2\n",
    "data1.loc[data1['rCSI']>18, 'rCSICat_aux']=3\n",
    "\n",
    "rcs = {1: \"No or low coping\",  2: \"Stress coping\" ,3: \"Crisis and above crisis coping\"}\n",
    "\n",
    "data1['rCSICat'] = data1['rCSICat_aux'].replace(rcs)\n",
    "\n",
    "\n",
    "# Compute FCS_CARI\n",
    "data1['FCS_CARI_aux']=0\n",
    "data1.loc[data1['FCSCat']=='Acceptable', 'FCS_CARI_aux']=1\n",
    "data1.loc[(data1['FCSCat']=='Acceptable') & (data1['rCSICat']=='Crisis and above crisis coping'), 'FCS_CARI_aux']=2\n",
    "data1.loc[data1['FCSCat']=='Borderline', 'FCS_CARI_aux']=3\n",
    "data1.loc[data1['FCSCat']=='Poor', 'FCS_CARI_aux']=4\n",
    "\n",
    "fcs_cari = {1: 'Acceptable', 2: 'Acceptable food consumption but severe food coping', 3: 'Borderline', 4: 'Poor'}\n",
    "\n",
    "data1['FCS_CARI'] = data1['FCS_CARI_aux'].replace(fcs_cari)\n",
    "data1['FCS_CARI'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## *1. STRESS STRATEGIES: sold household assets, purchased on credit, spent savings, borrowed money\n",
    "\n",
    "data1['stress_coping_EN'] = ((data1['LcsEN_stress_DomAsset'] == 2) | \n",
    "                            (data1['LcsEN_stress_DomAsset'] == 3) |\n",
    "                            (data1['LcsEN_stress_CrdtFood'] == 2) |\n",
    "                            (data1['LcsEN_stress_CrdtFood'] == 3) |\n",
    "                            (data1['LcsEN_stress_Saving'] == 2) |\n",
    "                            (data1['LcsEN_stress_Saving'] == 3) |\n",
    "                            (data1['LcsEN_stress_BorrowCash'] == 2) |\n",
    "                            (data1['LcsEN_stress_BorrowCash'] == 3)).astype(int)\n",
    "\n",
    "data1['stress_coping_EN'].value_counts()\n",
    "\n",
    "# *2. CRISIS STRATEGIES: sold productive assets, reduced expenses for health/education, withdrew \n",
    "# children from school \n",
    "\n",
    "data1['crisis_coping_EN'] = ((data1['LcsEN_crisis_ProdAsset'] == 2) | \n",
    "                            (data1['LcsEN_crisis_ProdAsset'] == 3) |\n",
    "                            (data1['LcsEN_crisis_HealthEdu'] == 2) |\n",
    "                            (data1['LcsEN_crisis_HealthEdu'] == 3) |\n",
    "                            (data1['LcsEN_crisis_AgriInput'] == 2) |\n",
    "                            (data1['LcsEN_crisis_AgriInput'] == 3) |\n",
    "                            (data1['LcsEN_crisis_Seed'] == 2) |\n",
    "                            (data1['LcsEN_crisis_Seed'] == 3)).astype(int)\n",
    "data1['crisis_coping_EN'].value_counts()\n",
    "\n",
    "# *3. EMERGENCY STRATEGIES: sold house/land, begged, engaged in illegal income activities\n",
    "\n",
    "data1['emergency_coping_EN'] = ((data1['LcsEN_em_Migrate'] == 2) | \n",
    "                                (data1['LcsEN_em_Migrate'] == 3) |\n",
    "                                (data1['LcsEN_em_Begged'] == 2) |\n",
    "                                (data1['LcsEN_em_Begged'] == 3) |\n",
    "                                (data1['LcsEN_em_IllegalAct'] == 2) |\n",
    "                                (data1['LcsEN_em_IllegalAct'] == 3) |\n",
    "                                (data1['LcsEN_em_ResAsset'] == 2) |\n",
    "                                (data1['LcsEN_em_ResAsset'] == 3)).astype(int)\n",
    "\n",
    "data1['emergency_coping_EN'].value_counts()\n",
    "# compute Max coping \n",
    "data1['Max_coping_behaviorEN_aux'] = 1\n",
    "data1.loc[data1['stress_coping_EN'] == 1, 'Max_coping_behaviorEN_aux'] = 2\n",
    "data1.loc[data1['crisis_coping_EN'] == 1, 'Max_coping_behaviorEN_aux'] = 3\n",
    "data1.loc[data1['emergency_coping_EN'] == 1, 'Max_coping_behaviorEN_aux'] = 4\n",
    "\n",
    "# Assuming you have a dictionary to map the labels\n",
    "label_max_cop = {\n",
    "    1: \"HH not adopting coping strategies\",\n",
    "    2: \"Stress coping strategies\",\n",
    "    3: \"Crisis coping strategies\",\n",
    "    4: \"Emergency coping strategies\"\n",
    "}\n",
    "\n",
    "# Mapping the labels\n",
    "data1['Max_coping_behaviorEN'] = data1['Max_coping_behaviorEN_aux'].map(label_max_cop)\n",
    "\n",
    "data1['Max_coping_behaviorEN'].value_counts()\n",
    "\n",
    "# Household Income Change \n",
    "\n",
    "# First, initialize 'HH_incomechange' as 0\n",
    "data1['HH_incomechange_aux'] = 0\n",
    "\n",
    "# Now, apply the conditions using pandas' loc method\n",
    "data1.loc[data1['HH_incomechangetype'] == 'Income increased', 'HH_incomechange_aux'] = 1\n",
    "data1.loc[data1['HH_incomechangetype'] == 'No Change', 'HH_incomechange_aux'] = 2\n",
    "data1.loc[data1['HHIncChTypeDec'] == 3, 'HH_incomechange_aux'] = 3\n",
    "data1.loc[data1['HHIncChTypeDec'] == 2, 'HH_incomechange_aux'] = 4\n",
    "data1.loc[data1['HHIncChTypeDec'] == 1, 'HH_incomechange_aux'] = 5\n",
    "\n",
    "# Display the frequency table of 'HH_incomechange'\n",
    "print(data1['HH_incomechange_aux'].value_counts())\n",
    "\n",
    "# Assuming you have a dictionary to map the labels\n",
    "label_income = {\n",
    "    1: \"Income Increased\",\n",
    "    2: \"No Change\",\n",
    "    3: \"Reduced by less than 25%\",\n",
    "    4: \"Reduced by more than 25% and less than 50%\",\n",
    "    5: \"Reduced by more than 50%\"\n",
    "}\n",
    "\n",
    "# Mapping the labels\n",
    "data1['HH_incomechange'] = data1['HH_incomechange_aux'].map(label_income)\n",
    "\n",
    "data1['HH_incomechange'].value_counts()\n",
    "\n",
    "# creat Incomegroup variables\n",
    "data1['HHIncFirst'] = data1['HH_mainincomesource'].replace({\n",
    "    1:1, 2:1, 3:1, 4:1, 5:1, 17:1,\n",
    "    6:2, 7:2, 8:2, 9:2, 10:2,\n",
    "    11:3, 12:3, 13:3, 14:3, 15:3, 16:3, -88:3})\n",
    "\n",
    "\n",
    "# First, initialize 'CARI_Inc' as 0\n",
    "data1['CARI_Inc_aux'] = 0\n",
    "\n",
    "# Now, apply the conditions using pandas' loc method\n",
    "data1.loc[(data1['HHIncFirst'] == 1) & (data1['HH_incomechange_aux'] < 3), 'CARI_Inc_aux'] = 1\n",
    "data1.loc[(data1['HHIncFirst'] == 1) & (data1['HH_incomechange_aux'] >= 3), 'CARI_Inc_aux'] = 2\n",
    "data1.loc[(data1['HHIncFirst'] == 2) & (data1['HH_incomechange_aux'] < 3), 'CARI_Inc_aux'] = 2\n",
    "data1.loc[(data1['HHIncFirst'] == 2) & (data1['HH_incomechange_aux'] >= 3), 'CARI_Inc_aux'] = 3\n",
    "data1.loc[data1['HHIncFirst'] == 3, 'CARI_Inc_aux'] = 4\n",
    "\n",
    "# Display the frequency table of 'CARI_Inc'\n",
    "print(data1['CARI_Inc_aux'].value_counts())\n",
    "\n",
    "# Assuming you have a dictionary to map the labels\n",
    "label_cari_in = {\n",
    "    1: \"Regular employment (formal labour or self-employed) – income no change or increased\",\n",
    "    2: \"Regular employment but reduced income or informal source, income no change/ no decrease\",\n",
    "    3: \"Informal labour /remittances but reduced income\",\n",
    "    4: \"No income, dependent on assistance or support\"\n",
    "}\n",
    "\n",
    "# Mapping the labels\n",
    "data1['CARI_Inc'] = data1['CARI_Inc_aux'].map(label_cari_in)\n",
    "\n",
    "data1['CARI_Inc'].value_counts()\n",
    "\n",
    "## calculating rCARI\n",
    "\n",
    "data1['rCARI_mean'] = (data1['FCSCat_aux'] * 0.25) + (data1['rCSICat_aux'] * 0.25) + (data1['CARI_Inc_aux'] * 0.25) + (data1['Max_coping_behaviorEN_aux'] * 0.25)\n",
    "\n",
    "\n",
    "data1['rCARI_aux'] = 0\n",
    "\n",
    "# Now, apply the conditions using pandas' loc method\n",
    "data1.loc[data1['rCARI_mean'] < 1.5, 'rCARI_aux'] = 1\n",
    "data1.loc[(data1['rCARI_mean'] >= 1.5) & (data1['rCARI_mean'] <= 2.25), 'rCARI_aux'] = 2\n",
    "data1.loc[(data1['rCARI_mean'] >= 2.5) & (data1['rCARI_mean'] <= 3.25), 'rCARI_aux'] = 3\n",
    "data1.loc[data1['rCARI_mean'] >= 3.5, 'rCARI_mean'] = 4\n",
    "\n",
    "\n",
    "# Assuming you have a dictionary to map the labels\n",
    "label_rCARI = {\n",
    "    1: \"Food secure\",\n",
    "    2: \"Marginally food secure\",\n",
    "    3: \"Moderately food insecure\",\n",
    "    4: \"Severely food insecure\"\n",
    "}\n",
    "\n",
    "# Mapping the labels\n",
    "data1['rCARI'] = data1['rCARI_aux'].map(label_rCARI)\n",
    "\n",
    "data1['rCARI'].value_counts()\n",
    "\n",
    "# compute aggregates of key micronutrient consumption – vitamin, iron and protein \n",
    "col_recode = ['FCSStap', 'FCSVeg', 'FCSFruit', 'FCSPr', 'FCSPulseNut', 'FCSDairy', 'FCSFat', 'FCSSugar']\n",
    "data1[col_recode] = data1[col_recode].replace('.',0)\n",
    "col_recode2 = ['FCSDairy', 'FCSPr', 'FCSPrMeatF', 'FCSPrMeatO', 'FCSPrFish', 'FCSDriedFish', 'FCSPrEgg', 'FCSVeg', 'FCSVegOrg', 'FCSVegGre', 'FCSFruit', 'FCSFruitOrg', 'FCSFruitsOth']\n",
    "data1[col_recode2] = data1[col_recode2].replace('.',0)\n",
    "\n",
    "data1['FGVitA'] = data1['FCSDairy'] + data1['FCSPrMeatO'] + data1['FCSPrEgg'] + data1['FCSVegOrg'] + data1['FCSVegGre'] + data1['FCSFruitOrg'] + data1['FCSFruitsOth']\n",
    "data1['FGProtein'] = data1['FCSPulseNut'] + data1['FCSDairy'] + data1['FCSPrMeatF'] + data1['FCSPrMeatO'] + data1['FCSPrFish'] + data1['FCSPrEgg']\n",
    "data1['FGHIron'] = data1['FCSPrMeatF'] + data1['FCSPrMeatO'] + data1['FCSPrFish']\n",
    "\n",
    "# Recoding the key micronutrient consumptions\n",
    "bins = [0,1,6,data1['FGVitA'].max()]\n",
    "bins1 = [0,1,6,data1['FGProtein'].max()]\n",
    "bins2 = [0,1,6,data1['FGHIron'].max()]\n",
    "labels = [1,2,3]\n",
    "\n",
    "data1['FGVitACat_aux'] = pd.cut(data1['FGVitA'], bins=bins, labels=labels, include_lowest=True)\n",
    "data1['FGProteinCat_aux'] = pd.cut(data1['FGProtein'], bins = bins1, labels=labels, include_lowest=True)\n",
    "data1['FGHIron_aux'] = pd.cut(data1['FGHIron'], bins = bins2, labels=labels, include_lowest=True)\n",
    "\n",
    "# Add value labels \n",
    "# Assuming you have a dictionary to map the labels\n",
    "label_nut = {\n",
    "    1: \"0 days\",\n",
    "    2: \"1-6 days\",\n",
    "    3: \"7 days\"\n",
    "}\n",
    "\n",
    "# Mapping the labels\n",
    "data1['FGVitACat'] = data1['FGVitACat_aux'].map(label_nut)\n",
    "data1['FGProteinCat'] = data1['FGProteinCat_aux'].map(label_nut)\n",
    "data1['FGHIronCat'] = data1['FGHIron_aux'].map(label_nut)\n",
    "\n",
    "## show the values\n",
    "col_print = ['FGVitA','FGProtein', 'FGHIron','FGVitACat', 'FGProteinCat', 'FGHIronCat']\n",
    "# iterate over each column\n",
    "for column in col_print:\n",
    "    print(data1[column].value_counts())\n",
    "    \n",
    "# Create two variable dummy\n",
    "data1['HH_expenditurechange_3months'] = None\n",
    "data1['HH_inomechange_6months'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93517cff",
   "metadata": {},
   "source": [
    "# Create the varibales for KPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef397bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (1181, 215)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the data:\", data1.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60545a14",
   "metadata": {},
   "source": [
    "# Assuming you have a dictionary to map the labels\n",
    "label = {\n",
    "    1: \"Regular income\",\n",
    "    2: \"Informal Income\",\n",
    "    3: \"Depend on assistance or support\"\n",
    "}\n",
    "\n",
    "# Mapping the labels\n",
    "data1['HHIncFirst'] = data1['HHIncFirst'].map(label)\n",
    "\n",
    "data1['HHIncFirst'].value_counts()\n",
    "\n",
    "common_col = list(set(source_data.columns) & set(data_main.columns))\n",
    "source_data = source_data[common_col]\n",
    "data_main = data_main[common_col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f15034",
   "metadata": {},
   "source": [
    "# working with reshaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd531b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCollection_Date               datetime64[ns]\n",
      "DIVISION_Name                             object\n",
      "DISTRICT_Name                            float64\n",
      "DISTRICT_Name                            float64\n",
      "Incomegroup                               object\n",
      "HHShock_6MNo0                            float64\n",
      "HHShock_6MLossofemploymentre             float64\n",
      "HHShock_6MSicknesshealthexp              float64\n",
      "HHShock_6MDeathofhouseholdme             float64\n",
      "HHShock_6MHighfoodprices4                float64\n",
      "HHShock_6MHighfuelpricetrans             float64\n",
      "HHShock_6MNaturalhazardsfloo             float64\n",
      "HHShock_6MPoorharvest7                   float64\n",
      "HHShock_6MElectricitygascuts             float64\n",
      "HHShock_6MInsecuritythefts9              float64\n",
      "HHShock_6MIrregularunsafedrin            float64\n",
      "HHShock_6MLackofaccesstocre              float64\n",
      "HHShock_6MDebttoreinburse12              float64\n",
      "HHShock_6MRentpayment13                  float64\n",
      "HHShock_6MEnvironmentalproblem           float64\n",
      "HHShock_6MOthers88                       float64\n",
      "dtype: object\n",
      "DataCollection_Date               object\n",
      "DIVISION_Name                     object\n",
      "DISTRICT_Name                     object\n",
      "DISTRICT_Name                     object\n",
      "Incomegroup                       object\n",
      "HHShock_6MNo0                     object\n",
      "HHShock_6MLossofemploymentre      object\n",
      "HHShock_6MSicknesshealthexp       object\n",
      "HHShock_6MDeathofhouseholdme      object\n",
      "HHShock_6MHighfoodprices4         object\n",
      "HHShock_6MHighfuelpricetrans      object\n",
      "HHShock_6MNaturalhazardsfloo      object\n",
      "HHShock_6MPoorharvest7            object\n",
      "HHShock_6MElectricitygascuts      object\n",
      "HHShock_6MInsecuritythefts9       object\n",
      "HHShock_6MIrregularunsafedrin     object\n",
      "HHShock_6MLackofaccesstocre       object\n",
      "HHShock_6MDebttoreinburse12       object\n",
      "HHShock_6MRentpayment13           object\n",
      "HHShock_6MEnvironmentalproblem    object\n",
      "HHShock_6MOthers88                object\n",
      "dtype: object\n",
      "Date_Div_Income    object\n",
      "variable           object\n",
      "value              object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saidul.haq\\AppData\\Local\\Temp\\ipykernel_33740\\4566025.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HH_Shock['IncomeGroup_count']= HH_Shock['IncomeGroup'].groupby(HH_Shock['IncomeGroup']).transform('count')\n"
     ]
    }
   ],
   "source": [
    "file_save = \"C:/Users/saidul.haq/OneDrive - World Food Programme/Saidul/1.mVAM dashboard project/BD mVAM data/pivot data/Pivot.xlsx\"\n",
    "\n",
    "Pivot_data =data1[['DataCollection_Date','DIVISION_Name','DISTRICT_Name', 'DISTRICT_Name', 'Incomegroup', 'HHShock_6MNo0', 'HHShock_6MLossofemploymentre', 'HHShock_6MSicknesshealthexp', 'HHShock_6MDeathofhouseholdme', 'HHShock_6MHighfoodprices4', 'HHShock_6MHighfuelpricetrans', 'HHShock_6MNaturalhazardsfloo', 'HHShock_6MPoorharvest7', 'HHShock_6MElectricitygascuts', 'HHShock_6MInsecuritythefts9', 'HHShock_6MIrregularunsafedrin', 'HHShock_6MLackofaccesstocre', 'HHShock_6MDebttoreinburse12', 'HHShock_6MRentpayment13', 'HHShock_6MEnvironmentalproblem', 'HHShock_6MOthers88']]\n",
    "\n",
    "## Convert the data type of new table\n",
    "print(Pivot_data.dtypes)\n",
    "Pivot_data = Pivot_data.astype(str)\n",
    "print(Pivot_data.dtypes)\n",
    "\n",
    "## combine somes columns to keep it Id variable in the reshpaed table\n",
    "Pivot_data['Date_Div_Income'] = Pivot_data[['DataCollection_Date', 'DIVISION_Name', 'DISTRICT_Name', 'Incomegroup']].agg(','.join, axis=1)\n",
    "Pivot_data\n",
    "\n",
    "## Now reshpae the table\n",
    "reshaped_HHShock = pd.melt(Pivot_data, id_vars= 'Date_Div_Income',  value_vars=['HHShock_6MNo0', 'HHShock_6MLossofemploymentre', 'HHShock_6MSicknesshealthexp', 'HHShock_6MDeathofhouseholdme', 'HHShock_6MHighfoodprices4', 'HHShock_6MHighfuelpricetrans', 'HHShock_6MNaturalhazardsfloo', 'HHShock_6MPoorharvest7', 'HHShock_6MElectricitygascuts', 'HHShock_6MInsecuritythefts9', 'HHShock_6MIrregularunsafedrin', 'HHShock_6MLackofaccesstocre', 'HHShock_6MDebttoreinburse12', 'HHShock_6MRentpayment13', 'HHShock_6MEnvironmentalproblem', 'HHShock_6MOthers88'])\n",
    "\n",
    "print(reshaped_HHShock.dtypes)\n",
    "\n",
    "reshaped_HHShock[['Date','Division', 'District', 'District_Name', 'IncomeGroup']] = reshaped_HHShock.Date_Div_Income.str.split(\",\", expand=True)\n",
    "\n",
    "reshaped_HHShock[['aux','HH_Shock']] = reshaped_HHShock.variable.str.split(\"_\", expand=True)\n",
    "HH_Shock=reshaped_HHShock[['Date','Division', 'IncomeGroup', 'HH_Shock', 'value']]\n",
    "\n",
    "HH_Shock['IncomeGroup_count']= HH_Shock['IncomeGroup'].groupby(HH_Shock['IncomeGroup']).transform('count')\n",
    "\n",
    "HH_Shock.to_excel(file_save, sheet_name=\"Mar24\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024a836",
   "metadata": {},
   "source": [
    "# Working with main data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44416472",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path =\"C:/Users/saidul.haq/OneDrive - World Food Programme/Saidul/1.mVAM dashboard project/BD mVAM data/BGD_WFP_mVAM_round_combine_tableau_datasource_new.xlsx\"\n",
    "# Read Excel data into a DataFrame\n",
    "source_data= pd.read_excel(excel_file_path, sheet_name='BGD_WFP_mVAM_round_combine', engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04551601",
   "metadata": {},
   "source": [
    "## Use this code when you need to remove rows from source_data, Possible use case may be sometimes combined two data but #results find not ok, and need to remove the data from source file \n",
    "\n",
    "sample_dates = source_data['DataCollection_Date'].sample(5)  # You can adjust the sample size as needed\n",
    "\n",
    "# Print the sampled dates to see their format\n",
    "print(\"Sampled dates:\")\n",
    "for date in sample_dates:\n",
    "    print(date)\n",
    "\n",
    "\n",
    "# Filter out February 2024 data\n",
    "feb_2024_data = source_data[(source_data['DataCollection_Date'].dt.month==7) & (source_data['DataCollection_Date'].dt.year == 2024)]\n",
    "\n",
    "# Remove February 2024 data from the original dataframe\n",
    "source_data = source_data[~((source_data['DataCollection_Date'].dt.month==7) & (source_data['DataCollection_Date'].dt.year == 2024))]\n",
    "\n",
    "# Extract year from DataCollection_Date\n",
    "source_data['Year'] = source_data['DataCollection_Date'].dt.year\n",
    "\n",
    "# Count values by year\n",
    "yearly_counts = source_data['Year'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Counts by year:\")\n",
    "print(yearly_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f10c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now keep the variable for final main data sources for tableau \n",
    "variables_keep = VarMapping['KeepInAnalyzed'].dropna().tolist() \n",
    "\n",
    "# Filter out the if any column that do not exist in the data1, it's beacuse the data collection form changes time to time\n",
    "existing_cols = [col for col in variables_keep if col in data1.columns]\n",
    "\n",
    "# Create the final dataFrame with only the existing columns\n",
    "data_main = data1[existing_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220fa224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (31064, 47)\n"
     ]
    }
   ],
   "source": [
    "# Now keep the variable for final main data sources for tableau \n",
    "variables_to_keep = VarMapping['KeepInAnalyzed'].dropna().tolist()\n",
    "data_source = source_data[variables_to_keep].copy()\n",
    "print(\"Shape of the data:\", data_source.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98822ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (32245, 47)\n"
     ]
    }
   ],
   "source": [
    "# Append the new data to the existing data\n",
    "combined_df = pd.concat([data_source, data_main], axis=0, ignore_index=True)\n",
    "# Print the shape of the data\n",
    "print(\"Shape of the data:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a28cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Write the combined data to the same Excel file, replacing the existing data\n",
    "combined_df.to_excel(excel_file_path, sheet_name=\"BGD_WFP_mVAM_round_combine\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381bae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rCARI\n",
      "Marginally food secure      916\n",
      "Moderately food insecure    241\n",
      "Food secure                  23\n",
      "Name: count, dtype: int64\n",
      "rCSI\n",
      "0.0     452\n",
      "2.0     157\n",
      "3.0     136\n",
      "1.0      98\n",
      "4.0      76\n",
      "5.0      58\n",
      "6.0      52\n",
      "7.0      49\n",
      "9.0      16\n",
      "12.0     15\n",
      "10.0     12\n",
      "8.0       9\n",
      "13.0      7\n",
      "14.0      6\n",
      "16.0      6\n",
      "18.0      6\n",
      "11.0      5\n",
      "15.0      4\n",
      "22.0      3\n",
      "20.0      3\n",
      "17.0      2\n",
      "24.0      2\n",
      "21.0      2\n",
      "19.0      2\n",
      "27.0      1\n",
      "23.0      1\n",
      "29.0      1\n",
      "Name: count, dtype: int64\n",
      "rCARI_mean\n",
      "2.25    256\n",
      "2.00    237\n",
      "1.50    212\n",
      "1.75    211\n",
      "2.50    163\n",
      "2.75     70\n",
      "1.25     23\n",
      "3.00      8\n",
      "4.00      1\n",
      "Name: count, dtype: int64\n",
      "rCARI_aux\n",
      "2    916\n",
      "3    241\n",
      "1     23\n",
      "0      1\n",
      "Name: count, dtype: int64\n",
      "FCSCat_aux\n",
      "3    831\n",
      "2    344\n",
      "1      6\n",
      "Name: count, dtype: int64\n",
      "rCSICat_aux\n",
      "1    843\n",
      "2    323\n",
      "3     15\n",
      "Name: count, dtype: int64\n",
      "CARI_Inc_aux\n",
      "1    444\n",
      "2    388\n",
      "3    264\n",
      "4     85\n",
      "Name: count, dtype: int64\n",
      "Max_coping_behaviorEN_aux\n",
      "3    449\n",
      "2    365\n",
      "1    348\n",
      "4     19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## show the values\n",
    "col_print = ['rCARI','rCSI','rCARI_mean','rCARI_aux', 'FCSCat_aux','rCSICat_aux', 'CARI_Inc_aux', 'Max_coping_behaviorEN_aux']\n",
    "# iterate over each column\n",
    "for column in col_print:\n",
    "    print(data1[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c9211f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rCARI\n",
       "Marginally food secure      916\n",
       "Moderately food insecure    241\n",
       "Food secure                  23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['rCARI'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1cc18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
